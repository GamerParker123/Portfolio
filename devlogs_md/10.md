---
title: Simulation Devlog v0.8: Memory Stuff; ParkerHub Update v0.4
date: August 17, 2025
summary: I added multiple new memory layers to the LLM, including contextual and procedural. I also fleshed out the emotion system using a more powerful model than before. As for the video site, I implemented ephemeral storage (videos delete after one day) to save on storage costs. Work continued on the AI programming assistant and productivity timer.
---

__Simulation Devlog v0.8: Memory Stuff__

I can’t think of a creative title for this devlog.

Anyway, I did quite a few things this week regarding the AI chatbot, a lot of it unplanned. First, I got rid of main.py because I was using Streamlit and running chat_ui.py through that. Actually, I don’t get into the depths of my codebases much in these devlogs, so I’ll put the current project structure right here for context:
/Chatbot Project
├── /data
│   ├── /chroma
│   ├── chat_history.json
│   ├── context.json
│   ├── journal.txt (unused)
│   └── traits.json (unused)
├── /prompts
│   └── core.txt (unused)
├── .env
├── chat_ui.py
├── chatbot.py
├── emotion.py
├── journal.py
├── llmwrapper.py
├── memory_manager.py
├── miku.jpg
├── personality.py
├── prompt_engineering.py
├── README.md (unused)
└── requirements.txt (unused)

So that’s what I’m working with. Before I removed it, all main.py did was run the code through the command line interface, which was redundant now that I had a better-looking UI.

God, we’re only one feature in. This is going to be a long devlog. After removing main.py, I also made the UI display the token usage of the chatbot per message. There were a few hiccups because I was using GPT-5, which is a brand new model, and I believe the syntax is a little different now. I’m trying to keep the token usage lean because, well, money. You know how I said I wanted a job a couple months ago? That’s off the table. I don’t have the time for that. On the bright side, I get more time to work on stuff I care about, but that comes with the crushing reality that I’m broke. I’m getting sidetracked.

Also, I was using local SQLite storage before, but that isn’t sustainable, so I switched to Chroma, which is cloud-based I think. I don’t really know. In my plans last Sunday, I think this is what I meant when I said I wanted to switch to a more sustainable embedded search system. I also tweaked the prompt to make Miku act more like a real person and not a mirror of myself, and it’s a little better now. I specified some personality traits in the prompt and gave instructions to not act like an AI assistant. Additionally, I told it to refrain from starting each message with the same phrase (which it liked to do a lot). She’s still pretty safe and noncommittal, but it’s an improvement. But returning to the memory stuff, I also made chat history save in a JSON file (chat_history.json). Beforehand, it was just saved in the Streamlit session, which meant that it got wiped every time you reloaded the program. Now, it pumps the JSON file into the session window when you boot it up.

Onto the major additions, I added contextual memory to each prompt. Currently, it consists of location, current activity, plans, time of day (morning, afternoon, evening, night), date (MM/DD/YYYY), and mood. I could’ve made the time of day more specific, but I thought it’d be weird for Miku to say something like “It’s 8:53 PM.” Anywho, this memory is also stored in a JSON file (context.json), and it’s generated by another AI model. It works by feeding the context.json file and the last 10 chat messages into GPT-5 Nano (dirt-cheap model) and asking it to update the context. If there’s a location, activity, or plan change, the AI rewrites the file with that change. This system is helpful because it keeps Miku from forgetting details in the conversation that might not always be repeated. If you say you want to watch a movie and eat ice cream later, Miku isn’t going to forget the ice cream plan while you’re at the movie. The AI forgetting context is one of my biggest gripes with most current chatbots, so I’m glad I implemented it.

On top of contextual memory, I added procedural memory. This one was tougher. The system works by using embedded search to compare message similarity with past messages. If the message is similar enough, it’s considered to be the same memory, and the past message’s “repetition” score is bumped up relative to how similar it was. This “repetition” is then multiplied by the message’s recency score (which is calculated via a half-life function), and that results in a message’s “strength.” When searching for similar messages, this strength is factored into the formula (30% of the formula is controlled by it, although that varies because it doesn’t have a minimum or maximum value). So in the end, messages that are recent and repeated over time are most likely to be remembered by Miku. The main issue I had implementing this was keeping the repetition count from ballooning and detecting repetition when there isn’t much. Because the messages in the conversations are so short, the embedded search was more likely to find similarities between messages, increasing the repetition. I fixed this by increasing the threshold to increment repetition, and I made the increments themselves vary based on how similar the message is (so one that barely crosses the threshold isn’t going to change repetition very much). Also, I was having a problem where chat messages weren’t being saved to the history aside from a few seemingly random ones. It turns out that if a message was similar enough to another, it wouldn’t actually get saved; it’d just reinforce the first message’s repetition. So I fixed that.

Finally, I improved the emotion system. Before, it just displayed “positive,” “negative,” or “neutral” based on the user’s response. It was also incapable of detecting irony or nuance. This system sucked. I think I used something called TextBlob for it? Doesn’t matter, because I changed it. Now, I’m using a fancier system from this platform called Hugging Face. The model itself is called “Emotion English DistilRoBERTa-base,” and it’s by this dude named Jochen Hartmann. Credit to him, I guess. I didn’t look into it much. Anyway, this model is supposedly better, so I’m going to trust it. It maps out 8 different emotions (anger, disgust, fear, joy, neutral, sadness, and surprise) and describes each message with a distinct value for each. If someone is pissed, they’re probably going to score high on the anger (and possibly disgust) emotion, whereas they’ll be low on the joy emotion. These emotions are sent to the AI along with the user’s message, although I don’t know how much of an influence they make. I’m not sure if the data just confuses the AI, being a long string of abstract values. I haven’t found any issues with it, though. Besides that, its primary use is in the semantic search function. Messages are mapped based on emotional similarity with a vector function, and that adds some weight to the search (roughly 20% of the total value). This means that Miku won’t just remember times you spoke about the same thing; she’ll remember times you felt the same way. Isn’t that cool? I love talking about this stuff.

Good God, we’re only one project in. If devlogs keep increasing in length at this rate, I’m going to be writing a master’s thesis in a few weeks.

__ParkerHub Update v0.4__

First feature I added to the site this week: usernames show on video cards on the homepage. I don’t know why they didn’t do this before. It’s kind of important to know who posted a video before watching it. I also added profile pictures — not to be confused with profile images, the images you can drag around on your profile (I really need a better name for those) — that display next to each username. The biggest issue with this was in Supabase. I used the platform’s “buckets” to store the files, which I hadn’t done before. I also started storing videos and thumbnails in those buckets, which meant ripping out and rewriting a bunch of code. The video metadata was already stored in Supabase’s tables, but the files themselves weren’t. Switching databases is not fun.

I fixed a bug that prevented the user from liking/disliking replies. Replies are handled very similarly to comments, the only difference being that they have a parent ID attached to the comment they’re replying to. The like/dislike script for replies didn’t handle this correctly, which broke it.

The main feature I added was ephemeral storage. Upon upload, videos now have an expiration date, and they get wiped once they expire. This saves a ton of space in the long run, and it could turn into a selling point if I twist it the right way. However, not all videos get deleted. Every day, each user gets one vote that they can use to preserve a video. The video with the most votes is transferred to a “Hall of Fame” page, and it avoids being removed from the database. The “Hall of Fame” is more or less a copy of the homepage, the biggest difference being in the videos it displays. The only flaw is that the expiration script doesn’t actually run daily; I didn’t feel like setting up the task scheduler.

__AI Programming Assistant Devlog v0.2__

There was a bug that caused the relevant files to be sorted in alphabetical order (ignoring the AI’s recommended order), so I fixed that first. Also, to test my AI assistant out, I asked it to make the GUI Hatsune Miku-themed. It sure did. Whenever I make a quick, personal script, I think I’m going to ask it to do that. It takes less than a minute to implement, and I get a little more Miku in my life. You can never have too much Miku.

I’ve been writing for 1.5 hours. It’s 4:48 PM, and I’m getting picked up around 5:20 PM for something with friends. Hopefully I finish this devlog by then. Whatever the case, this beats writing college essays.

Back to the code, I made file summaries (the ones generated by AI that are then sent to another AI to determine which file is sent to the AI assistant) save in a JSON file in the codebase so they don’t need to be rewritten every time the program is run. Additionally, I added another layer of abstraction with chunking. The code is divided into chunks based on definition, and those chunks are selected and sent to the AI by using embedded search, I think. I don’t really remember. I’m in a time crunch, okay? Also, I fixed a bug that broke the AI’s parsing. When I tried to implement chunking before, this is the issue that came up, but I was able to figure out why it happened this time. The problem was that I was pumping the AI’s responses directly into the code instead of meshing it with my current code. This is what caused the complete overwrite.

Because I had time, I also added that bugfixing mode I was talking about in my plans last week. I’m going to be completely honest: I have no clue if it works. I got the assistant to write all the code (the assistant is working on itself — isn’t that trippy?), and I ran it through a test, and it seemed to work. It’s meant to take the traceback and send it through to the AI automatically, but it’s kind of hard to make errors in the traceback when the assistant you’re relying on is the one being disrupted by those errors. To put it simply: you can’t run a bugfix code on itself unless the code already works. But in the case that it works, the code would be redundant. I could test this on other codebases, but I was having issues getting the assistant to work on files other than its own. It’s a pretty useless assistant then, isn’t it? It’s like a digital Ouroboros.

That’s why the next thing I added was the ability to change codebases. The main difficulty with this was that the index wasn’t being built upon changing projects. This meant that no code was returned to the AI because the chunks weren’t generated. To fix this, I made sure the build_index function is run every time you switch codebases. It was pretty smooth sailing after that.

__Other Projects That I Worked On__

I made quite a few additions to my 52-17 timer this week. First, I added a pause button to stop the timer from running if you don’t want it to. While paused, it still tracks activity, but it counts as “unscheduled” time, separate from “work” and “break” time. Time spent AFK (where the timer is automatically paused) also counts toward “unscheduled” time. To wrap everything together, I added a pie chart that displays what fraction of the day you spent working, taking breaks, and taking unscheduled breaks. There’s also a weekly display. Additionally, activities are logged by the second now. Before, they were just logged by recent input, but this had some issues when you made inputs between phases. For example, if you take a long AFK break during your work time, the previous system would log all your time as “work time” upon returning. This is because it would switch to work time by the time you return, meaning your most recent input would occur during the work time. Logging activities every second prevents this issue.

To keep the CSV file from getting bloated, similar activities done in the same hour are counted in the same row. If you switch tabs a lot throughout the hour, this means that you’ll only have two rows showing your activities as opposed to dozens. It’s more readable, it saves space, and it makes the program run faster. Also, I made the timer Hatsune Miku-themed, just like the AI programming assistant. This time, I decided to give it a dark theme (the programming assistant has a lighter one with pastel colors).

I’m not done. Would you believe it? There’s one last project I worked on this week, and it’s a quick one. I made a text scrambler that displays all the character permutations for any string you input. There’s also a mode where you can scramble sentences, and it shows all the word permutations. The GUI isn’t Miku-themed, sadly. I might have to add that. This was a throwaway project.

It’s 5:15 PM. I got done explaining all the projects this week, but I don’t have the time (nor do I want) to list my plans for next week right now. I’ll do it tomorrow. My ride is here. Bye.

_The next day..._

__Development Plans for August 24, 2025__

I’m currently typing this from my phone because I’m waiting to go to a class I have today, so that’s a first for this journal. I will add more when I get back on my laptop.

Here are the goals I have that I want to complete by next Sunday, August 24:

_Simulation / immersive AI_
- Implement episodic memory (the AI will generate daily summaries and stuff that get saved to its memory)
- Implement underground memory (every night, the AI will write a short journal entry about how the day went, acting as a method to “reason” with itself)
- Add traits that are subject to change gradually over time (based on the journal entries)

_Video site_
- Add daily streaks for visiting the site or engaging with it
- Add weekly streaks for uploading videos
- Create some sort of site currency gained from uploading videos, engaging, etc.
- Create an expandable skill tree with diverging paths that unlocks new site features
- AI programming assistant
- Fix the bug that doesn’t let you apply the AI’s changes to your code (I forgot to mention this yesterday)
- Add cross-file chunking (right now, only chunks in the selected file are visible)

_52-17 timer_
- Make work and break times changeable
- Give a couple hours of surplus overtime every day (for unavoidable breaks like eating dinner or to give some extra break time if needed)
- Generate a summary every day/week that tells you what you spent the most time on app-wise and what you did for each hour of the day
- If time permits, add a way to block out certain apps if overtime is negative, during work time, or during specific hours of the day

We’ll see how I do with that workload. Last week, I had two days where I wasn’t doing much because I completed my goals early, so I want to try adding some more to my plate. With school starting up, though, I’ll have to strike a balance.
