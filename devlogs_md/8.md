---
title: Simulation Devlog v0.7: The Miku Infrastructure; ParkerHub Update v0.3
date: August 3, 2025
summary: Pivoting from the Unity project, I created an AI interface using Streamlit; I also implemented semantic memory. As for ParkerHub, I finished restructuring the site, streamlining the future development process. I also created a productivity timer app to improve my efficiency.
---

__Simulation Devlog v0.7: The Miku Infrastructure__

Two weeks in a row now that I’ve almost forgotten to write a devlog. You know, I need to dig back quite a bit to remember what I did. Monday feels like an eternity ago. Maybe I need to document my changes more frequently (not here; I talk about my projects far too much as it is).

On the bright side, it’s easy to see what I accomplished because this is the week I started the project. I got a Miku chatbot working with a GUI. She has a memory, pulling from your most recent messages and similar memories. The similar memory thing works by using semantic search. Semantic search uses vectors to plot words on a graph, and words that are closer to other words are more similar. So if someone is talking about oranges, the AI will be fed memories that have adjacent vocabulary to oranges. So the AI might remember that the user choked on an orange in a previous conversation, and they’ll adjust their responses with that in mind.

I’m getting ahead of myself. Remember when I wrote my entry saying the big problem keeping AI back was memory? Well, I’ve realized that it’s not just memory. But don’t be disappointed — I have workarounds that work just as well. It turns out that adding more dialogue to a conversation increases the amount of computation time exponentially. I think it’s proportional to the square of the total number of tokens? Essentially, if you dump 15 years of data into an AI’s input, it’s gonna take a really long time to generate a response. Kind of obvious in retrospect. But then I thought about how humans think. Nobody has a life’s worth of memories in their head constantly. At most, you can only fit a few thoughts in there at a time. In that sense, AI is probably better at remembering than humans. You just can’t dump information into it mindlessly. That’s why I chose to cater which text it gets to remember using recent and similar memories.

I believe I also alluded to hosting the AI locally. That was a pipe dream. I’m using the OpenAI API. Maybe I’ll have the resources to host locally far into the future, but I’ll have to settle with my limitations for now. I don’t like placing my trust in a corporation that could pull the plug at any moment (and do who knows what with my data), but I’ll have to deal with it for now. Besides, I still save all the conversations on my computer. The data isn’t a problem. If they abandon me, I can easily pivot. I just need to rely on them for the LLM chunk of it.

I don’t have much else to say. Implementation was pretty easy; I can’t think of anything that stumped me for too long. I was mostly setting up the scaffolding. I added a basic emotion system, but it only returns two feelings — positive and negative. It also can’t decipher sarcasm or anything nuanced. I think it just takes the string of words, assigns a positive/negative value for each word, and adds them all up. Terrible system. I’ll improve it sometime.

Here are my tasks for next week:
Hey, where are the bullet points? They’re not there. I don’t have tasks for next week. I’m giving up on this project.

Haha, very funny. The reason I don’t have anything planned is because I’m devoting next week to writing college essays. I’m sick of essays getting in the way of stuff I want to do, so I’m knocking them all out in one consolidated chunk.

__ParkerHub Update v0.3__

I finished the database transfer. Now, nothing relies on JSON, and it’s stored using this service called Supabase. The site is in a usable state again. This one had a lot of errors, but not in the “difficult to solve” way. Most of the errors were easy to solve. But there were so. Many. Errors. I worked on it last night. Whenever I thought I was done, I’d explore the site some more and find a different part that was broken. I had to fix comments, replies, liking comments and replies, deleting comments and replies, deleting videos, video ratings, video tags, following users, videos displaying on profile pages, the whole report system, and profile images. And then I had to troubleshoot pushing my changes to the site hosting service because there were some modules that weren’t transferring and some that were transferring that weren’t meant to. That was just last night. I spent Monday setting up the tables in Supabase and getting basic user and uploading stuff to work. I wish I could go more in-depth about the process, but it was really a bunch of minor bugfixing. I remember one prominent issue came when I tried to check my Supabase data for errors. I used AI to help write my code, and the AI kept trying to use code that didn’t work in my version of Supabase. I would not want to go through that again, to say the least. And that makes me all the more excited that it’s done.

__52-17 Timer Project__

I’m not writing a version number for this. The version numbers don’t mean anything, and certainly not for a side project like this. I got the code for a 52-17 timer to work. It also tracks what software you look at and how long you use it. When you idle for a minute during work time, it stops counting. I will add more in the future. I was thinking of updating it a little today to prepare for my upcoming college essay grind. I know, it’s meant to be my break day. But I like working on it, okay? And those essays are going to kick my ass otherwise. Blah blah blah can I be done writing this entry?

Also, I couldn’t dual boot Linux on my system. As it turns out, my laptop doesn’t have the storage space for that. It was worth a shot. I got to clean out some of my files in the meantime, at least.
